## Homework 2: 02-Workflow-Orchestration-with-Kestra

Welcome to **Week 2** of the **Data Engineering Zoomcamp**! This week focuses on Workflow Orchestration, using Kestra to automate data pipelines and manage ETL workflows efficiently. The homework extends the existing workflow to process additional data while answer some qestions related to the workflow orchestration.

---

## üìã **Assignment Overview**  

During the course, we processed green and yellow NYC taxi data for the year 2019. The assignment is to extend the existing workflows to include data for subsequent dates. To do that, we use the Kestra‚Äôs **backfill functionality** and **subflows** to automate processing for both **Yellow** and **Green** taxi datasets.

---

## üìù **Quiz Answers**  

### **1Ô∏è‚É£ Uncompressed File Size for December 2020 (Yellow Taxi Data)**  
‚úÖ **Answer:** `128.3 MB`  

### **2Ô∏è‚É£ Rendered Value of `file` Variable (Green Taxi, April 2020)**  
‚úÖ **Answer:** `green_tripdata_2020-04.csv`  

### **3Ô∏è‚É£ Total Rows in Yellow Taxi Data (2020)**  
‚úÖ **Answer:** `24,648,499`  

### **4Ô∏è‚É£ Total Rows in Green Taxi Data (2020)**  
‚úÖ **Answer:** `1,734,051`  

### **5Ô∏è‚É£ Total Rows in Yellow Taxi Data (March 2021)**  
‚úÖ **Answer:** `1,925,152`  

### **6Ô∏è‚É£ Configuring New York Timezone in Kestra Schedule**  
‚úÖ **Answer:** `Add a timezone property set to America/New_York in the Schedule trigger configuration`  

---

## üõ† **Tools Used**  

- **Kestra** ‚Üí Workflow orchestration  
- **BigQuery** ‚Üí Data warehouse for storing and analyzing data  
- **Google Cloud Storage (GCS)** ‚Üí Cloud-based data lake  
- **Docker & Docker Compose** ‚Üí Local environment setup  
